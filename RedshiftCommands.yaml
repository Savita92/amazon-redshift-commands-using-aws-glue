AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  DatabaseHostName:
    Description: The hostname on which the cluster accepts incoming connections.
    Type: String
  MasterUsername:
    Description: The user name which will be used to execute the SQL Script.
    Type: String
    AllowedPattern: "([a-z])([a-z]|[0-9])*"
  MasterUserPassword:
    Description: The password which will be used to execute the SQL Script.
    Type: String
    NoEcho: 'true'
  PortNumber:
    Description: The port number on which the cluster accepts incoming connections.
    Type: Number
    Default: '5439'
  DatabaseName:
    Description: The name of the database which will be used to execute the SQL Script.
      created
    Type: String
    Default: 'dev'
    AllowedPattern: "([a-z]|[0-9])+"
  VPC:
    Description: The VPC Redshift is running in.
    Type: AWS::EC2::VPC::Id
  Subnet:
    Description: The Subnet Redshift is running in.
    Type: AWS::EC2::Subnet::Id
  AvailabilityZone:
    Description: The Availability Zone Redshift is running in.
    Type: AWS::EC2::AvailabilityZone::Name
  RouteTable:
    Description: The Route Table associated to the subnet which Redshift is running in.
    Type: String
  SecurityGroup:
    Description: The Security Group which provides access to Redshift.
    Type: AWS::EC2::SecurityGroup::Id
  SampleJob:
    Description: If enabled, a sample job will be created which loads createa a customer_[stackname] table into the public schema.  The script for the job can be found here at s3://redshift-immersionday-labs/customer.sql
    Type: String
    AllowedValues:
      - true
      - false
    Default: false
Conditions:
  IsSampleJob:
    Fn::Equals:
    - Ref: SampleJob
    - true
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Network Details"
        Parameters:
          - VPC
          - AvailabilityZone
          - Subnet
          - SecurityGroup
          - RouteTable
      -
        Label:
          default: "Connection Details"
        Parameters:
          - DatabaseHostName
          - MasterUsername
          - MasterUserPassword
          - PortNumber
          - DatabaseName
      -
        Label:
          default: "Sample Execution"
        Parameters:
          - SampleJob
Resources:
  GlueBucket:
    Type: AWS::S3::Bucket
  SecretManagerVPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    #Not actual dependencies, just to ensure this resource is created last
    DependsOn:
      - GlueConnection
    Properties:
      SecurityGroupIds:
        - Ref: SecurityGroup
      PrivateDnsEnabled: true
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.secretsmanager'
      SubnetIds:
        - Ref: Subnet
      VpcEndpointType: Interface
      VpcId:
        Ref: VPC
  S3VPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    #Not actual dependencies, just to ensure this resource is created last
    DependsOn:
      - GlueConnection
    Properties:
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      PolicyDocument: '{
        "Version":"2012-10-17",
        "Statement":[{
          "Effect":"Allow",
          "Principal": "*",
          "Action":"*",
          "Resource":"*"
        }]
      }'
      VpcEndpointType: Gateway
      RouteTableIds:
        - !Ref RouteTable
      VpcId:
        Ref: VPC
  GlueLoadRedshiftRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
          Version: 2012-10-17
          Statement:
            -
              Effect: Allow
              Principal:
                Service:
                  - glue.amazonaws.com
                  - lambda.amazonaws.com
              Action:
                - sts:AssumeRole
      Path: /
      Policies:
          -
            PolicyName: LambdaLoadRedshiftPolicy
            PolicyDocument:
              Version: 2012-10-17
              Statement:
                -
                  Effect: Allow
                  Action:
                    - s3:*
                  Resource:
                    - !Sub "arn:aws:s3:::cloudformation-custom-resource-response-${AWS::Region}"
                    - !Sub "arn:aws:s3:::cloudformation-waitcondition-${AWS::Region}"
                    - !Sub "arn:aws:s3:::cloudformation-custom-resource-response-${AWS::Region}/*"
                    - !Sub "arn:aws:s3:::cloudformation-waitcondition-${AWS::Region}/*"
          -
            PolicyName: GlueGetSecretPolicy
            PolicyDocument :
              Version: 2012-10-17
              Statement:
                -
                  Effect: Allow
                  Action:
                    - secretsmanager:GetSecretValue
                  Resource:
                    - Ref: Secret
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
        - arn:aws:iam::aws:policy/CloudWatchLogsFullAccess
  Secret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Description: Secret for Redshift Command Glue Job.
      SecretString: !Sub
        - "{\"user\": \"${user}\", \"password\": \"${pass}\", \"host\": \"${host}\", \"database\": \"${db}\", \"port\": \"${port}\"}"
        - {user: !Ref MasterUsername, pass: !Ref MasterUserPassword, host: !Ref DatabaseHostName, db: !Ref DatabaseName, port: !Ref PortNumber}
  GlueConnection:
    Type: AWS::Glue::Connection
    Properties:
      CatalogId: !Ref AWS::AccountId
      ConnectionInput:
        Description: "Connect to VPC where Redshift is running."
        ConnectionType: "JDBC"
        ConnectionProperties: {
          "JDBC_CONNECTION_URL": "jdbc:redshift://host:9999/db",
          "USERNAME": "user",
          "PASSWORD": "password"
        }
        PhysicalConnectionRequirements:
          SecurityGroupIdList:
            - Ref: SecurityGroup
          SubnetId:
            Ref: Subnet
          AvailabilityZone:
            Ref: AvailabilityZone
  GlueJobRedshiftCommands:
    Type: AWS::Glue::Job
    DependsOn:
      - GlueConnection
    Properties:
      Name: RedshiftCommands
      Role: !GetAtt 'GlueLoadRedshiftRole.Arn'
      ExecutionProperty:
        MaxConcurrentRuns: 10
      Connections:
        Connections:
          - Ref: GlueConnection
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: !Sub
          - s3://${bucket}/RedshiftCommands.py
          - {bucket: !Ref GlueBucket}
      DefaultArguments:
        "--job-bookmark-option" : "job-bookmark-disable"
        "--TempDir" : !Sub
          - s3://${bucket}
          - {bucket: !Ref GlueBucket}
        "--enable-metrics" : ""
  LambdaGlueJobRedshiftCommands:
     Condition: IsSampleJob
     Type: "AWS::Lambda::Function"
     Properties:
       FunctionName: TriggerGlueLoadRedshiftCommand
       Role: !GetAtt 'GlueLoadRedshiftRole.Arn'
       Timeout: 300
       Code:
         ZipFile: |
           import json
           import boto3
           import cfnresponse
           import logging

           logging.basicConfig()
           logger = logging.getLogger(__name__)
           logger.setLevel(logging.INFO)

           glue = boto3.client('glue')

           def handler(event, context):
             logger.info(json.dumps(event))
             if event['RequestType'] != 'Create':
                 cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Data': 'NA'})
             else:
                 try:
                   sqlScript = event['ResourceProperties']['sqlScript']
                   secret = event['ResourceProperties']['secret']
                   params = event['ResourceProperties']['params']
                   jobName = event['ResourceProperties']['jobName']

                   response = glue.start_job_run(
                     JobName=jobName,
                     Arguments={
                       '--SQLScript':sqlScript,
                       '--Secret':secret,
                       '--Params':params})

                   message = 'Glue triggered successfully.'
                   cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Data': message})

                 except Exception as e:
                   message = 'Glue Job Issue'
                   logger.info(e)
                   cfnresponse.send(event, context, cfnresponse.FAILED, {'Data': 'message'})
       Handler: index.handler
       Runtime: python3.7
     DependsOn:
       - GlueLoadRedshiftRole
  LambdaFunctionS3Copy:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: S3Copy
      Timeout: 30
      Code:
        ZipFile: |
          import json
          import boto3
          import cfnresponse
          import logging

          logging.basicConfig()
          logger = logging.getLogger(__name__)
          logger.setLevel(logging.INFO)

          def handler(event, context):
            logger.info(json.dumps(event))
            s3 = boto3.client('s3')
            s3BucketTarget = event['ResourceProperties']['s3BucketTarget']
            s3Bucket = event['ResourceProperties']['s3Bucket']
            s3Object = event['ResourceProperties']['s3Object']

            if event['RequestType'] == 'Delete':
              try:
                s3.delete_object(Bucket=s3BucketTarget, Key=s3Object)
              except Exception as e:
                logger.info(e)

              logger.info('Delete Complete')
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Data': 'Delete complete'})

            else:
              try:
                s3.delete_object(Bucket=s3BucketTarget, Key=s3Object)
              except Exception as e:
                logger.info(e)
              try:
                s3.copy_object(Bucket=s3BucketTarget, CopySource=s3Bucket+"/"+s3Object, Key=s3Object)
                logger.info('Copy Complete')
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Data': 'Copy complete'})

              except Exception as e:
                logger.error(e)
                cfnresponse.send(event, context, cfnresponse.FAILED, {'Data': 'Copy failed'})

      Handler: index.handler
      Role:
        Fn::GetAtt: [GlueLoadRedshiftRole, Arn]
      Runtime: python3.7
    DependsOn:
      - GlueLoadRedshiftRole
  ImportPyScript:
    Type: Custom::CopyScript
    DependsOn:
      - LambdaFunctionS3Copy
      - GlueBucket
    Properties:
      ServiceToken:
        Fn::GetAtt : [LambdaFunctionS3Copy, Arn]
      s3BucketTarget:
        Ref: GlueBucket
      s3Bucket: 'redshift-immersionday-labs'
      s3Object: 'RedshiftCommands.py'
  InitGlueJobRedshiftCommands:
     Condition: IsSampleJob
     Type: Custom::InitGlueJobRedshiftCommands
     DependsOn:
       - LambdaGlueJobRedshiftCommands
       - GlueJobRedshiftCommands
       - SecretManagerVPCEndpoint
       - S3VPCEndpoint
       - ImportPyScript
     Properties:
       ServiceToken: !GetAtt 'LambdaGlueJobRedshiftCommands.Arn'
       sqlScript: !Sub
         - s3://${bucket}/customer.sql
         - {bucket: !Ref GlueBucket}
       secret: !Ref Secret
       params: !Sub "${AWS::StackName}"
       jobName: !Ref GlueJobRedshiftCommands
  ImportSqlScript:
    Type: Custom::CopyScript
    Condition: IsSampleJob
    DependsOn:
      - LambdaFunctionS3Copy
      - GlueBucket
    Properties:
      ServiceToken:
        Fn::GetAtt : [LambdaFunctionS3Copy, Arn]
      s3BucketTarget:
        Ref: GlueBucket
      s3Bucket: 'redshift-immersionday-labs'
      s3Object: 'customer.sql'
